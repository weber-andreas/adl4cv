\section{Neural Radiance Fields (NeRF)}
\begin{itemize}
    \item Neural Radiance Field (NeRF) is a neural network-based representation for 3D scenes that encodes both geometry and appearance.
    \item Enables photo-realistic novel view sybnthesis from a sparse set of input images.
    \item Now-standard approach to neural volumetric rendering.
\end{itemize}
\href{https://arxiv.org/abs/2003.08934}{Neural Radiance Fields (NeRF), Mildenhall et al., ECCV 2020}

\subsection{Core Idea}
\begin{itemize}
    \item NeRF represents a scene as a continuous 5D function:
    \[
    F_\theta : (x,y,z,\theta,\phi) \rightarrow (\sigma, \mathbf{c})
    \]
    \item 5D Input:
    \begin{itemize}
        \item $(x,y,z)$: 3D position
        \item $(\theta,\phi)$: viewing direction
    \end{itemize}
    \item Outputs (MLP):
    \begin{itemize}
        \item $\mathbf{c}$: emitted color at sample point perceived from direction
        \item $\sigma$: volume density (chance of absorption at specific location)
    \end{itemize}
    \item Volumetric rendering allows to reconstruct the final image using sampled points along camera rays.
    \item All steps are end-to-end differentiable, allowing training via gradient descent.
\end{itemize}

\begin{center}
    \includegraphics[width=\columnwidth]{images/NeRF_function.jpeg}
    \label{fig:NeRF_function}
\end{center}

\begin{itemize}
    \item Neural: Use a Neural Network as scene representation instead of explicit data structures.
    \item Volumetric: continuous, differentiable rendering model without concrete surface representation (similar to a cloud of particles).
    \item Rendering: compute color along rays through 3D space.
\end{itemize}

\begin{center}
    \includegraphics[width=0.8\columnwidth]{images/NeRF_bunny.jpeg}
    \label{fig:NeRF_bunny}
\end{center}

\subsection{Probabilistic Interpretation}

\begin{itemize}
    \item Ray traveling through scene hits a particle at distance $t$, where color $c(t)$ returned.
    \item Probability of hitting a particle at small interval $dt$ at distance $t$: $\sigma(t) dt$
    \item Probability of not hitting any particle before distance $t$ (Transmittance): $T(t)$
\end{itemize}
\begin{center}
    \includegraphics[width=0.9\columnwidth]{images/NeRF_probabilistic1.jpeg}
    \label{fig:NeRF_probabilistic}
\end{center}
\begin{itemize}
    \item P[no hit before t + dt] = P[no hit before t] * P[no hit at t]
    \item $T(t + dt) = T(t) * (1 - \sigma(t) dt)$
\end{itemize}
\[
T(t+dt) = T(t)\bigl(1 - \sigma(t)\,dt\bigr)
\]
Solve via Taylor Expansion:
\[
T(t+dt) \approx T(t) + T'(t)\,dt = T(t) - T(t)\sigma(t)\,dt
\]
\[
\frac{T'(t)}{T(t)}\,dt = -\sigma(t)\,dt
\]
\[
\int_{t_0}^{t} \frac{T'(s)}{T(s)}\,ds = -\int_{t_0}^{t} \sigma(s)\,ds
\]
\[
\log T(t) = -\int_{t_0}^{t} \sigma(s)\,ds
\]
\[
T(t) = \exp\!\left( -\int_{t_0}^{t} \sigma(s)\,ds \right)
\]
This gives P[first hit at t]:
\[
p(t) = T(t) \ * \sigma(t)dt
\]
To get the expected color returned by the ray, we need to integrate over all sampling points $t_i$ along the ray:
\[
C = \int_{t_0}^{t_n} T(t) \, \sigma(t) \, c(t) \, dt
\]
Note the nested integral, since $T(t)$ itself is an integral over density $\sigma(s)$.

\subsection{Discrete Integration (Approximation)}
Sampling at discrete points $t_i$ along the ray:
\[
 C = \int_{t_0}^{t_n} T(t) \, \sigma(t) \, c(t) \, dt \approx \sum_{i=1}^{N} T_i \, \alpha_i \, c_i
\]
This is differentiable w.r.t. $c$, $\sigma$.
Where: $\alpha_i$ are the length of the integral at sample point $t_i$:

\begin{center}
    \includegraphics[width=0.75\columnwidth]{images/NeRF_probabilistic_discrete.jpeg}
    \label{fig:NeRF_probabilistic_discrete}
\end{center}

How much light is blocked earlier along the ray:
\[
T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)
\]
How much light is contributed by ray segment $i$:
\[
\alpha_i = 1 - \exp(-\sigma_i \delta_i)
\]

Since the rendering weights $T_i$ and $\alpha_i$ are dependent on the points AND viewing direction:
\[
C \approx \sum_{i=1}^{N} \color{red}{T_i} \, \color{red}{\alpha_i} \, \color{black}{c_i}
\]
\[
w_i = T_i \, \alpha_i
\]

\textbf{Rendering Weight PDF}:
The distribution of weights can be used to compute expected depth and uncertainty:
\[
\bar{t} = \sum_{i=1}^{N}  w_i \, t_i
\]
where $t_i$ is just the from the camera to the sample point $i$.

\subsection{General Architecture}
\begin{center}
    \includegraphics[width=\columnwidth]{images/NeRF_general_architecture.jpeg}
    \label{fig:NeRF_general_architecture}
\end{center}
However, MLP are not required. There are works that use spares voxel grids and spherical harmonics.
\begin{center}
    \includegraphics[width=\columnwidth]{images/NeRF_alternative.jpeg}
    \label{fig:NeRF_alternative}
\end{center}

\subsection{Advantages}
\begin{itemize}
    \item Continuous, high-resolution scene representation.
    \item High-quality novel-view synthesis with realistic lighting and specularities.
    \item Compact representation: scene stored in MLP weights.
\end{itemize}


\subsection{Limitations (Original NeRF)}
\begin{itemize}
    \item Slow training and rendering due to MLP evaluations on every ray sample.
    \item Challenges with unknown or inaccurate camera poses
    \item Dynamic scenes and dynamic lighting are complicated to model.
\end{itemize}

\begin{center}
    \includegraphics[width=0.75\columnwidth]{images/NeRF_inaccurate_camera_poses.jpeg}
    \label{fig:NeRF_inaccurate_camera_poses}
\end{center}

\textbf{Appearance Embedding}
Serves as a robust solution to learn a latent representation of varying appearance conditions (lighting, weather, etc.)
\begin{center}
    \includegraphics[width=\columnwidth]{images/NeRF_apperance_embedding.jpeg}
    \label{fig:NeRF_apperance_embedding}
\end{center}

\subsection{NeRFs vs 3D Meshes}
NeRFs are good for \textbf{Reconstruction}:
\begin{itemize}
    \item Optimization-based
    \item Need good gradients
    \item Not just for surface representation but also for depth, normals
\end{itemize}

3D Meshes are good for \textbf{Rendering}:
\begin{itemize}
    \item Fast rendering of surfaces (only)
    \item Efficient
    \item No gradients needed
\end{itemize}