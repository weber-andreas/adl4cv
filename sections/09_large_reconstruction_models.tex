\section{Large Reconstruction Models (LRM)}

\textbf{Reconstruction vs. Generation}
\begin{itemize}
	\item \textbf{Reconstruction:} Focuses on creating accurate 3D representations from specific inputs \begin{itemize}
		      \item Image-to-3D
		      \item Text-to-3D
		      \item Video-to-3D
		      \item Multi-view-to-3D
	      \end{itemize}
	\item \textbf{Generation:} Involves creating new 3D content from learned distributions
\end{itemize}


\begin{itemize}
	\item \textbf{Shift to Feed-Forward Networks:} Unlike traditional reconstruction which often requires per-scene optimization, Large Reconstruction Models (LRMs) use Feed-Forward Networks trained on large datasets to generalize to new inputs.
	\item \textbf{Input and Output:} These models accept inputs like single images, text, or sparse views and output 3D representations such as Neural Radiance Fields (NeRF) or 3D Gaussian Splats.
\end{itemize}

\begin{center}
	\includegraphics[width=0.9\columnwidth]{images/lrm_feed_forward.jpeg}
	\label{fig:lrm_feed_forward}
\end{center}

\subsection{Generalizable Implicit Fields (pixelNeRF)}
Generalizes across scenes without per-scene optimization by learning a scene-prior directly from images.

\begin{center}
	\includegraphics[width=0.8\columnwidth]{images/pixel_nerf.jpeg}
	\label{fig:pixel_nerf}
\end{center}

\textbf{Mechanism:}
\begin{itemize}
	\item \textbf{Feature Extraction}: Uses a CNN encoder to extract features from one or few input images.
	\item \textbf{Feature Mapping}: Extracts a feature vector for any 3D point by projecting it onto the input image planes.
	\item \textbf{Interpolation}: Image features are bilinearly interpolated to determine the precise features at the projected coordinates.
\end{itemize}

\begin{center}
	\includegraphics[width=0.9\columnwidth]{images/pixelnerf_architecture.jpeg}
	\includegraphics[width=0.9\columnwidth]{images/pixelnerf_method.jpeg}
	\includegraphics[width=0.9\columnwidth]{images/pixelnerf_method2.jpeg}
	\label{fig:pixelnerf_architecture}
\end{center}

\textbf{Supervision:}
\begin{enumerate}
	\item Shoot camera rays from pixels of the target view.
	\item Sample points along these rays to determine spatial features.
	\item Predict color and opacity ($RGB, \sigma$) using an MLP based on the 3D position, view direction, and extracted image features.
	\item Trained using standard volume rendering and rendering loss against ground-truth images.
\end{enumerate}

\subsection{Scaling with Transformers (LRM)}
A high-capacity architecture designed for fast 3D reconstruction from a single image using transformer-based components.

\textbf{Architecture:}
\begin{itemize}
	\item \textbf{Image Encoder}: Utilizes a pretrained DINO ViT (Vision Transformer) with 12 layers and a 768-dimension hidden state.
	\item \textbf{Patchification}: The input image is "patchified" into a sequence of feature tokens with self-attention between patches.
	\item \textbf{Decoder}: A 16-layer Image-to-Triplane Decoder with a 1024-dimension processing space.
\end{itemize}

\begin{center}
	\includegraphics[width=0.8\columnwidth]{images/lrm_image_encoder.jpeg}
	\includegraphics[width=0.8\columnwidth]{images/lrm_triplane_decoder.jpeg}

	\label{fig:lrm_image_encoder}
\end{center}

\textbf{Conditioning Operations:}
\begin{itemize}
	\item \textbf{Cross-Attention}: With input image patches to control fine-grained geometric and color information.
	\item \textbf{Modulation}: With camera features (normalized extrinsics/intrinsics) to control the orientation and distortion of the reconstructed shape.
\end{itemize}

\textbf{Output Representation:}
\begin{itemize}
	\item \textbf{Triplane NeRF}: Outputs three axis-aligned feature planes with dimensions $3 \times (64 \times 64) \times 80$.
	\item \textbf{Point Querying}: To extract features for a 3D point, the point is projected onto each plane, and features are trilinearly interpolated.
	\item \textbf{MLP Mapping}: A 10-layer MLP maps interpolated point features to color and density for volumetric rendering.
\end{itemize}

\subsection{Real-Time Gaussian Reconstructors (GRM)}
Introduces efficiency by replacing volumetric querying with pixel-aligned 3D Gaussian Splatting (3DGS).

\textbf{Primitive:}
\begin{itemize}
	\item Predicts one 3D Gaussian per pixel, creating an attribute map.
	\item Attributes include depth (1), rotation (4), scale (3), opacity (1), and RGB (3) for a total of 12 dimensions.
\end{itemize}

\begin{center}
	\includegraphics[width=\columnwidth]{images/grm_overview.jpeg}
	\label{fig:grm_overview}
\end{center}

\textbf{Mechanism:}
\begin{itemize}
	\item \textbf{Encoder}: Processes input images and camera poses (encoded as Plücker rays) using a ViT encoder.
	\item \textbf{Upsampler}: A Transformer-based upsampler progressively reaches the original $H \times W$ resolution.
	\item \textbf{PixelShuffle}: Spatial dimensions are doubled while quadrupling channels to increase resolution efficiently.
	\item \textbf{Computation}: Employs Windowed Self-Attention to balance global information aggregation with feasible computation costs.
\end{itemize}

\textbf{Performance:}
\begin{itemize}
	\item Capable of reconstruction from input images in approximately 0.1 seconds.
	\item 3D Gaussians allow for high-fidelity, real-time novel-view rendering.
\end{itemize}

\subsection{Long-Sequence Scene Reconstruction}
Scales the input capability to handle wide-coverage scenes by processing long sequences of input views.

\textbf{Hybrid Architecture:}
\begin{itemize}
	\item \textbf{Hybrid Blocks}: Combines Mamba2 blocks for efficient linear scaling and Transformer blocks for global attention.
	\item \textbf{Scaling}: Mamba2 provides $O(L)$ scalability to denser views compared to $O(L^2)$ for standard self-attention.
	\item \textbf{Bi-directional Scanning}: Implements bi-directional scans (forward and backward) over token sequences to handle image data effectively.
\end{itemize}

\textbf{Capacity and Optimization:}
\begin{itemize}
	\item \textbf{Sequence Length}: Processes up to 32 images with Plücker rays in 1.35 seconds.
	\item \textbf{Token Merge}: An optional module reduces sequence length to $1/4$ using 2D convolution with stride 2 to manage long sequences.
	\item \textbf{Gaussian Pruning}: Prunes Gaussians by opacity at test-time to maintain efficiency during high-resolution scene reconstruction.
	\item \textbf{Training Objective}: Combines image loss (MSE + Perceptual), an opacity regularizer to reduce Gaussian count, and a depth regularizer to eliminate "floater" artifacts.
\end{itemize}
\newpage